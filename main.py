import datetime
import re
import requests
from bs4 import BeautifulSoup
import trafilatura

def get_clean_summary(url):
    try:
        # ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ìµœì  ì„¤ì •
        downloaded = trafilatura.fetch_url(url)
        if downloaded is None: return ""

        text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
        if not text or len(text) < 100: return ""

        # ë¶ˆí•„ìš”í•œ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ì œ
        text = re.sub(r'\s+', ' ', text).strip()

        # ë¬¸ì¥ ë‹¨ìœ„ ì¶”ì¶œ ë° ìš”ì•½
        sentences = text.split('. ')
        summary_sentences = []
        char_count = 0
        
        for sent in sentences:
            clean_sent = sent.strip()
            if len(clean_sent) < 25: continue # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            if not clean_sent.endswith('.'): clean_sent += '.'
            
            summary_sentences.append(clean_sent)
            char_count += len(clean_sent)
            if char_count > 350: break # í•µì‹¬ ë‚´ìš© ìœ„ì£¼ ìš”ì•½
        
        return ' '.join(summary_sentences)
    except:
        return ""

def get_naver_section_links(sid1, sid2=None):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ì—ì„œ ë¦¬ë””ë ‰ì…˜ ì—†ëŠ” ì§ì ‘ ê¸°ì‚¬ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    sid1: 105(IT), 101(ê²½ì œ), 102(ì‚¬íšŒ)
    sid2: 250(êµìœ¡)
    """
    links = []
    url = f"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={sid1}"
    if sid2:
        url += f"&sid2={sid2}"
        
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê¸°ì‚¬ ë§í¬ íŒ¨í„´(n.news.naver.com) íƒìƒ‰
        for a in soup.select('a[href*="article"]'):
            href = a['href']
            if 'n.news.naver.com/mnews/article/' in href:
                full_url = href.split('?')[0]
                if full_url not in links:
                    links.append(full_url)
            if len(links) >= 5: break 
    except Exception as e:
        print(f"Scraping Error: {e}")
    
    return links

def fetch_news():
    # êµìœ¡(sid2: 250) ì„¹ì…˜ì„ ëª…í™•íˆ ì§€ì •í•˜ì—¬ ëˆ„ë½ ë°©ì§€
    sections = {
        "ğŸ¤– ì¸ê³µì§€ëŠ¥ (AI)": {"sid1": "105"},
        "ğŸ’° ê²½ì œ": {"sid1": "101"},
        "ğŸ“ êµìœ¡": {"sid1": "102", "sid2": "250"}
    }
    
    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    
    markdown = f"""---
date: {today_str}
last_update: {now.strftime("%Y-%m-%d %H:%M:%S")}
type: insight
topic: [ì¸ê³µì§€ëŠ¥, ê²½ì œ, êµìœ¡]
---

# ğŸ“… {now.strftime('%Yë…„ %mì›” %dì¼(%a)')} í•µì‹¬ ë‰´ìŠ¤ ë¸Œë¦¬í•‘

"""
    
    for category, ids in sections.items():
        markdown += f"## {category}\n"
        links = get_naver_section_links(ids['sid1'], ids.get('sid2'))
        success_count = 0
        
        for link in links:
            if success_count >= 2: break
            summary = get_clean_summary(link)
            if not summary: continue
            
            markdown += f"### ğŸ”— [ê¸°ì‚¬ ì›ë¬¸ í™•ì¸í•˜ê¸°]({link})\n"
            markdown += f"> {summary}\n\n"
            success_count += 1
            
        if success_count == 0:
            markdown += "> í•´ë‹¹ ë¶„ì•¼ì˜ ìµœì‹  ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"

    filename = f"{today_str}_Daily_News_Briefing.md"
    return filename, markdown

if __name__ == "__main__":
    filename, content = fetch_news()
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
